{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.io.gfile import GFile\n",
    "import gin\n",
    "import os\n",
    "import jax\n",
    "import trax\n",
    "from trax.supervised import inputs\n",
    "\n",
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "from sentencepiece import SentencePieceProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up data and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be pushing the limits of just how many tokens we can fit on a single TPU device. The TPUs available in Colab have 8GB of memory per core, and 8 cores. We will set up a Reformer model that can fit a copy of \"Crime and Punishment\" on each of the 8 TPU cores (over 500,000 tokens per 8GB of memory)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import a copy of \"Crime and Punishment\", by Fyodor Dostoevsky\n",
    "with GFile('gs://trax-ml/reformer/crime-and-punishment-2554.txt') as f:\n",
    "  text = f.read()\n",
    "\n",
    "# The file read above includes metadata and licensing information.\n",
    "# For training our language model, we will only use the actual novel text.\n",
    "start = text.find('CRIME AND PUNISHMENT')  # skip header\n",
    "start = text.find('CRIME AND PUNISHMENT', start + 1)  # skip header\n",
    "start = text.find('CRIME AND PUNISHMENT', start + 1)  # skip translator preface\n",
    "end = text.rfind('End of Project')  # skip extra text at the end\n",
    "text = text[start:end].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sn/PhD_Dec_2019_Onwards/Experiments/NeuralLM/datas/bodo_raw/two/test.brx.txt','r', encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "    \n",
    "#text=text[1:3013812]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a BPE vocabulaary with 320 types. This mostly consists of single letters\n",
    "# and pairs of letters, but it has some common words and word pieces, too.\n",
    "#!gsutil cp gs://trax-ml/reformer/cp.320.* .\n",
    "\n",
    "TOKENIZER = SentencePieceProcessor()\n",
    "TOKENIZER.load('/home/sn/PhD_Dec_2019_Onwards/Experiments/NeuralLM/datas/pretrained_bodo/google-sentpiece/bodo-8k-sp-bpe.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "IDS = TOKENIZER.EncodeAsIds(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDS = onp.asarray(IDS, dtype=onp.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 114129\n"
     ]
    }
   ],
   "source": [
    "PAD_AMOUNT = 128 * 1024 - len(IDS)\n",
    "print(\"Number of tokens:\", IDS.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16943 114129\n"
     ]
    }
   ],
   "source": [
    "print(PAD_AMOUNT,len(IDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we see above, \"Crime and Punishment\" has just over half a million tokens with the BPE vocabulary we have selected.\n",
    "Normally we would have a dataset with many examples, but for this demonstration we fit a language model on the single novel only. We don't want the model to just memorize the dataset by encoding the words in its position embeddings, so at each training iteration we will randomly select how much padding to put before the text vs. after it.\n",
    "We have 8 TPU cores, so we will separately randomize the amount of padding for each core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(device count, tokens per device) =  (4, 131072)\n"
     ]
    }
   ],
   "source": [
    "# Set up the data pipeline.\n",
    "def my_inputs(n_devices):\n",
    "  while True:\n",
    "    inputs = []\n",
    "    mask = []\n",
    "    pad_amounts = onp.random.choice(PAD_AMOUNT, n_devices)\n",
    "    for i in range(n_devices):\n",
    "      inputs.append(onp.pad(IDS, (pad_amounts[i], PAD_AMOUNT - pad_amounts[i]),\n",
    "                            mode='constant'))\n",
    "      mask.append(onp.pad(onp.ones_like(IDS, dtype=onp.float32),\n",
    "                          (pad_amounts[i], PAD_AMOUNT - pad_amounts[i]),\n",
    "                          mode='constant'))\n",
    "    inputs = onp.stack(inputs)\n",
    "    mask = onp.stack(mask)\n",
    "    yield (inputs, inputs, mask)\n",
    "\n",
    "print(\"(device count, tokens per device) = \",\n",
    "      next(my_inputs(trax.math.device_count()))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure hyperparameters.\n",
    "gin.parse_config(\"\"\"\n",
    "import trax.layers\n",
    "import trax.models\n",
    "import trax.optimizers\n",
    "import trax.supervised.inputs\n",
    "import trax.supervised.trainer_lib\n",
    "\n",
    "# Parameters that will vary between experiments:\n",
    "# ==============================================================================\n",
    "train.model = @trax.models.ReformerLM\n",
    "# Our model will have 6 layers, alternating between the LSH attention proposed\n",
    "# in the Reformer paper and local attention within a certain context window.\n",
    "n_layers = 6\n",
    "attn_type = [\n",
    "  @TimeBinCausalAttention,\n",
    "  @LSHCausalAttention,  \n",
    "  @TimeBinCausalAttention,\n",
    "  @LSHCausalAttention,\n",
    "  @TimeBinCausalAttention,\n",
    "  @LSHCausalAttention,\n",
    "  ]\n",
    "share_qk = False  # LSHCausalAttention ignores this flag and always shares q & k\n",
    "n_heads = 2\n",
    "attn_kv = 64\n",
    "dropout = 0.05\n",
    "n_tokens = 131072\n",
    "\n",
    "# Parameters for MultifactorSchedule:\n",
    "# ==============================================================================\n",
    "MultifactorSchedule.constant = 0.01\n",
    "MultifactorSchedule.factors = 'constant * linear_warmup * cosine_decay'\n",
    "MultifactorSchedule.warmup_steps = 100\n",
    "MultifactorSchedule.steps_per_cycle = 900\n",
    "\n",
    "# Parameters for Adam:\n",
    "# ==============================================================================\n",
    "Adam.weight_decay_rate=0.0\n",
    "Adam.b1 = 0.86\n",
    "Adam.b2 = 0.92\n",
    "Adam.eps = 1e-9\n",
    "\n",
    "# Parameters for TimeBinCausalAttention:\n",
    "# ==============================================================================\n",
    "TimeBinCausalAttention.bin_length = 64\n",
    "TimeBinCausalAttention.dropout = 0.05\n",
    "TimeBinCausalAttention.n_bins = None\n",
    "TimeBinCausalAttention.share_qk = %share_qk\n",
    "\n",
    "# Parameters for LSHCausalAttention:\n",
    "# ==============================================================================\n",
    "LSHCausalAttention.allow_duplicate_attention = False\n",
    "LSHCausalAttention.attend_across_buckets = True\n",
    "LSHCausalAttention.rehash_each_round = True\n",
    "LSHCausalAttention.data_rotation = False\n",
    "LSHCausalAttention.n_bins = 4096\n",
    "LSHCausalAttention.n_buckets = 8192\n",
    "LSHCausalAttention.factorize_hash = [64, 128]\n",
    "LSHCausalAttention.n_hashes = 1\n",
    "LSHCausalAttention.one_rng = False\n",
    "LSHCausalAttention.hard_k = 0\n",
    "LSHCausalAttention.dropout = 0.0\n",
    "LSHCausalAttention.drop_for_hash_rate = 0.0\n",
    "LSHCausalAttention.max_len_for_inference = 2048\n",
    "LSHCausalAttention.bucket_capacity_for_inference = 64\n",
    "\n",
    "# Parameters for ReformerLM:\n",
    "# ==============================================================================\n",
    "ReformerLM.attention_type = %attn_type\n",
    "ReformerLM.d_attention_key = %attn_kv\n",
    "ReformerLM.d_attention_value = %attn_kv\n",
    "ReformerLM.d_model = 256\n",
    "ReformerLM.d_ff = 128\n",
    "ReformerLM.dropout = %dropout\n",
    "ReformerLM.ff_activation = @trax.layers.Relu\n",
    "ReformerLM.max_len = %n_tokens\n",
    "ReformerLM.mode = 'train'\n",
    "ReformerLM.n_heads = %n_heads\n",
    "ReformerLM.n_layers = %n_layers\n",
    "ReformerLM.vocab_size = 8000\n",
    "ReformerLM.share_qk = %share_qk\n",
    "ReformerLM.axial_pos_shape = (128, 1024)\n",
    "ReformerLM.d_axial_pos_embs= (64, 192)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from /home/sn/train_dir/2/model.pkl at step 1\n"
     ]
    }
   ],
   "source": [
    "# Set up a Trainer.\n",
    "output_dir = os.path.expanduser('~/train_dir/2/')\n",
    "!rm -f ~/train_dir/model.pkl  # Remove old model\n",
    "trainer = trax.supervised.Trainer(\n",
    "    model=trax.models.ReformerLM,\n",
    "    loss_fn=trax.layers.CrossEntropyLoss,\n",
    "    optimizer=trax.optimizers.Adam,\n",
    "    lr_schedule=trax.lr.MultifactorSchedule,\n",
    "    inputs=trax.supervised.inputs.Inputs(my_inputs),\n",
    "    output_dir=output_dir,\n",
    "    has_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      2: Ran 1 train steps in 48.17 secs\n",
      "Step      2: Evaluation\n",
      "Step      2: train                   accuracy |  0.00010733\n",
      "Step      2: train                       loss |  9.01715088\n",
      "Step      2: train         neg_log_perplexity |  9.01715279\n",
      "Step      2: train weights_per_batch_per_core |  114129.00000000\n",
      "Step      2: eval                    accuracy |  0.00014676\n",
      "Step      2: eval                        loss |  9.01628017\n",
      "Step      2: eval          neg_log_perplexity |  9.01628017\n",
      "Step      2: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step      2: Finished evaluation\n"
     ]
    }
   ],
   "source": [
    "# Run one training step, to make sure the model fits in memory.\n",
    "# The first time trainer.train_epoch is called, it will JIT the entire network\n",
    "# architecture, which takes around 2 minutes. The JIT-compiled model is saved\n",
    "# so subsequent runs will be much faster than the first.\n",
    "trainer.train_epoch(n_steps=1, n_eval_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step     11: Ran 9 train steps in 42.75 secs\n",
      "Step     11: Evaluation\n",
      "Step     11: train                   accuracy |  0.05587537\n",
      "Step     11: train                       loss |  7.44203568\n",
      "Step     11: train         neg_log_perplexity |  7.44203615\n",
      "Step     11: train weights_per_batch_per_core |  114129.00000000\n",
      "Step     11: eval                    accuracy |  0.05587537\n",
      "Step     11: eval                        loss |  7.44202852\n",
      "Step     11: eval          neg_log_perplexity |  7.44202805\n",
      "Step     11: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step     11: Finished evaluation\n",
      "\n",
      "Step     21: Ran 10 train steps in 5.88 secs\n",
      "Step     21: Evaluation\n",
      "Step     21: train                   accuracy |  0.07501161\n",
      "Step     21: train                       loss |  6.94360447\n",
      "Step     21: train         neg_log_perplexity |  6.94360447\n",
      "Step     21: train weights_per_batch_per_core |  114129.00000000\n",
      "Step     21: eval                    accuracy |  0.07511237\n",
      "Step     21: eval                        loss |  6.94419956\n",
      "Step     21: eval          neg_log_perplexity |  6.94419861\n",
      "Step     21: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step     21: Finished evaluation\n",
      "\n",
      "Step     31: Ran 10 train steps in 5.89 secs\n",
      "Step     31: Evaluation\n",
      "Step     31: train                   accuracy |  0.10134365\n",
      "Step     31: train                       loss |  5.96059847\n",
      "Step     31: train         neg_log_perplexity |  5.96059799\n",
      "Step     31: train weights_per_batch_per_core |  114129.00000000\n",
      "Step     31: eval                    accuracy |  0.10134365\n",
      "Step     31: eval                        loss |  5.96603203\n",
      "Step     31: eval          neg_log_perplexity |  5.96603155\n",
      "Step     31: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step     31: Finished evaluation\n",
      "\n",
      "Step     41: Ran 10 train steps in 5.90 secs\n",
      "Step     41: Evaluation\n",
      "Step     41: train                   accuracy |  0.15822664\n",
      "Step     41: train                       loss |  4.90981150\n",
      "Step     41: train         neg_log_perplexity |  4.90981197\n",
      "Step     41: train weights_per_batch_per_core |  114129.00000000\n",
      "Step     41: eval                    accuracy |  0.15843475\n",
      "Step     41: eval                        loss |  4.91079044\n",
      "Step     41: eval          neg_log_perplexity |  4.91079092\n",
      "Step     41: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step     41: Finished evaluation\n",
      "\n",
      "Step     51: Ran 10 train steps in 5.90 secs\n",
      "Step     51: Evaluation\n",
      "Step     51: train                   accuracy |  0.21258619\n",
      "Step     51: train                       loss |  4.12906981\n",
      "Step     51: train         neg_log_perplexity |  4.12906981\n",
      "Step     51: train weights_per_batch_per_core |  114129.00000000\n",
      "Step     51: eval                    accuracy |  0.21179761\n",
      "Step     51: eval                        loss |  4.13871765\n",
      "Step     51: eval          neg_log_perplexity |  4.13871861\n",
      "Step     51: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step     51: Finished evaluation\n",
      "\n",
      "Step     61: Ran 10 train steps in 5.91 secs\n",
      "Step     61: Evaluation\n",
      "Step     61: train                   accuracy |  0.24082835\n",
      "Step     61: train                       loss |  3.68372774\n",
      "Step     61: train         neg_log_perplexity |  3.68372774\n",
      "Step     61: train weights_per_batch_per_core |  114129.00000000\n",
      "Step     61: eval                    accuracy |  0.24197617\n",
      "Step     61: eval                        loss |  3.67171001\n",
      "Step     61: eval          neg_log_perplexity |  3.67171001\n",
      "Step     61: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step     61: Finished evaluation\n",
      "\n",
      "Step     71: Ran 10 train steps in 5.91 secs\n",
      "Step     71: Evaluation\n",
      "Step     71: train                   accuracy |  0.28395936\n",
      "Step     71: train                       loss |  3.25111771\n",
      "Step     71: train         neg_log_perplexity |  3.25111747\n",
      "Step     71: train weights_per_batch_per_core |  114129.00000000\n",
      "Step     71: eval                    accuracy |  0.28465596\n",
      "Step     71: eval                        loss |  3.24659920\n",
      "Step     71: eval          neg_log_perplexity |  3.24659944\n",
      "Step     71: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step     71: Finished evaluation\n",
      "\n",
      "Step     81: Ran 10 train steps in 5.91 secs\n",
      "Step     81: Evaluation\n",
      "Step     81: train                   accuracy |  0.30511087\n",
      "Step     81: train                       loss |  2.99259067\n",
      "Step     81: train         neg_log_perplexity |  2.99259090\n",
      "Step     81: train weights_per_batch_per_core |  114129.00000000\n",
      "Step     81: eval                    accuracy |  0.30449095\n",
      "Step     81: eval                        loss |  3.00067854\n",
      "Step     81: eval          neg_log_perplexity |  3.00067854\n",
      "Step     81: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step     81: Finished evaluation\n",
      "\n",
      "Step     91: Ran 10 train steps in 5.91 secs\n",
      "Step     91: Evaluation\n",
      "Step     91: train                   accuracy |  0.34561989\n",
      "Step     91: train                       loss |  2.66990352\n",
      "Step     91: train         neg_log_perplexity |  2.66990376\n",
      "Step     91: train weights_per_batch_per_core |  114129.00000000\n",
      "Step     91: eval                    accuracy |  0.34656614\n",
      "Step     91: eval                        loss |  2.66201305\n",
      "Step     91: eval          neg_log_perplexity |  2.66201305\n",
      "Step     91: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step     91: Finished evaluation\n",
      "\n",
      "Step    101: Ran 10 train steps in 5.91 secs\n",
      "Step    101: Evaluation\n",
      "Step    101: train                   accuracy |  0.38867199\n",
      "Step    101: train                       loss |  2.40703535\n",
      "Step    101: train         neg_log_perplexity |  2.40703511\n",
      "Step    101: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    101: eval                    accuracy |  0.39050108\n",
      "Step    101: eval                        loss |  2.39800334\n",
      "Step    101: eval          neg_log_perplexity |  2.39800334\n",
      "Step    101: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    101: Finished evaluation\n",
      "\n",
      "Step    111: Ran 10 train steps in 5.91 secs\n",
      "Step    111: Evaluation\n",
      "Step    111: train                   accuracy |  0.47192651\n",
      "Step    111: train                       loss |  1.97019339\n",
      "Step    111: train         neg_log_perplexity |  1.97019339\n",
      "Step    111: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    111: eval                    accuracy |  0.47282022\n",
      "Step    111: eval                        loss |  1.96368086\n",
      "Step    111: eval          neg_log_perplexity |  1.96368098\n",
      "Step    111: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    111: Finished evaluation\n",
      "\n",
      "Step    121: Ran 10 train steps in 5.91 secs\n",
      "Step    121: Evaluation\n",
      "Step    121: train                   accuracy |  0.55164987\n",
      "Step    121: train                       loss |  1.61170316\n",
      "Step    121: train         neg_log_perplexity |  1.61170316\n",
      "Step    121: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    121: eval                    accuracy |  0.55091166\n",
      "Step    121: eval                        loss |  1.61949730\n",
      "Step    121: eval          neg_log_perplexity |  1.61949730\n",
      "Step    121: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    121: Finished evaluation\n",
      "\n",
      "Step    131: Ran 10 train steps in 5.92 secs\n",
      "Step    131: Evaluation\n",
      "Step    131: train                   accuracy |  0.61989719\n",
      "Step    131: train                       loss |  1.34241807\n",
      "Step    131: train         neg_log_perplexity |  1.34241807\n",
      "Step    131: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    131: eval                    accuracy |  0.61683488\n",
      "Step    131: eval                        loss |  1.35320628\n",
      "Step    131: eval          neg_log_perplexity |  1.35320616\n",
      "Step    131: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    131: Finished evaluation\n",
      "\n",
      "Step    141: Ran 10 train steps in 5.91 secs\n",
      "Step    141: Evaluation\n",
      "Step    141: train                   accuracy |  0.69345212\n",
      "Step    141: train                       loss |  1.04977584\n",
      "Step    141: train         neg_log_perplexity |  1.04977560\n",
      "Step    141: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    141: eval                    accuracy |  0.68957061\n",
      "Step    141: eval                        loss |  1.06218743\n",
      "Step    141: eval          neg_log_perplexity |  1.06218743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    141: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    141: Finished evaluation\n",
      "\n",
      "Step    151: Ran 10 train steps in 5.92 secs\n",
      "Step    151: Evaluation\n",
      "Step    151: train                   accuracy |  0.74781162\n",
      "Step    151: train                       loss |  0.84602916\n",
      "Step    151: train         neg_log_perplexity |  0.84602916\n",
      "Step    151: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    151: eval                    accuracy |  0.74928588\n",
      "Step    151: eval                        loss |  0.84323359\n",
      "Step    151: eval          neg_log_perplexity |  0.84323353\n",
      "Step    151: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    151: Finished evaluation\n",
      "\n",
      "Step    161: Ran 10 train steps in 5.91 secs\n",
      "Step    161: Evaluation\n",
      "Step    161: train                   accuracy |  0.79256809\n",
      "Step    161: train                       loss |  0.67137605\n",
      "Step    161: train         neg_log_perplexity |  0.67137605\n",
      "Step    161: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    161: eval                    accuracy |  0.79303241\n",
      "Step    161: eval                        loss |  0.67059916\n",
      "Step    161: eval          neg_log_perplexity |  0.67059928\n",
      "Step    161: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    161: Finished evaluation\n",
      "\n",
      "Step    171: Ran 10 train steps in 5.93 secs\n",
      "Step    171: Evaluation\n",
      "Step    171: train                   accuracy |  0.83453155\n",
      "Step    171: train                       loss |  0.52605784\n",
      "Step    171: train         neg_log_perplexity |  0.52605784\n",
      "Step    171: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    171: eval                    accuracy |  0.83498275\n",
      "Step    171: eval                        loss |  0.52358013\n",
      "Step    171: eval          neg_log_perplexity |  0.52358013\n",
      "Step    171: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    171: Finished evaluation\n",
      "\n",
      "Step    181: Ran 10 train steps in 5.92 secs\n",
      "Step    181: Evaluation\n",
      "Step    181: train                   accuracy |  0.86166751\n",
      "Step    181: train                       loss |  0.41962600\n",
      "Step    181: train         neg_log_perplexity |  0.41962600\n",
      "Step    181: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    181: eval                    accuracy |  0.86144400\n",
      "Step    181: eval                        loss |  0.42067596\n",
      "Step    181: eval          neg_log_perplexity |  0.42067590\n",
      "Step    181: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    181: Finished evaluation\n",
      "\n",
      "Step    191: Ran 10 train steps in 5.93 secs\n",
      "Step    191: Evaluation\n",
      "Step    191: train                   accuracy |  0.88144553\n",
      "Step    191: train                       loss |  0.35038406\n",
      "Step    191: train         neg_log_perplexity |  0.35038409\n",
      "Step    191: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    191: eval                    accuracy |  0.88120019\n",
      "Step    191: eval                        loss |  0.35014129\n",
      "Step    191: eval          neg_log_perplexity |  0.35014129\n",
      "Step    191: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    191: Finished evaluation\n",
      "\n",
      "Step    201: Ran 10 train steps in 5.92 secs\n",
      "Step    201: Evaluation\n",
      "Step    201: train                   accuracy |  0.89620078\n",
      "Step    201: train                       loss |  0.29496804\n",
      "Step    201: train         neg_log_perplexity |  0.29496804\n",
      "Step    201: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    201: eval                    accuracy |  0.89622927\n",
      "Step    201: eval                        loss |  0.29673824\n",
      "Step    201: eval          neg_log_perplexity |  0.29673818\n",
      "Step    201: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    201: Finished evaluation\n",
      "\n",
      "Step    211: Ran 10 train steps in 5.92 secs\n",
      "Step    211: Evaluation\n",
      "Step    211: train                   accuracy |  0.90761983\n",
      "Step    211: train                       loss |  0.25434485\n",
      "Step    211: train         neg_log_perplexity |  0.25434488\n",
      "Step    211: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    211: eval                    accuracy |  0.90744239\n",
      "Step    211: eval                        loss |  0.25562182\n",
      "Step    211: eval          neg_log_perplexity |  0.25562182\n",
      "Step    211: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    211: Finished evaluation\n",
      "\n",
      "Step    221: Ran 10 train steps in 5.93 secs\n",
      "Step    221: Evaluation\n",
      "Step    221: train                   accuracy |  0.91571814\n",
      "Step    221: train                       loss |  0.22761062\n",
      "Step    221: train         neg_log_perplexity |  0.22761065\n",
      "Step    221: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    221: eval                    accuracy |  0.91545528\n",
      "Step    221: eval                        loss |  0.22883254\n",
      "Step    221: eval          neg_log_perplexity |  0.22883256\n",
      "Step    221: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    221: Finished evaluation\n",
      "\n",
      "Step    231: Ran 10 train steps in 5.94 secs\n",
      "Step    231: Evaluation\n",
      "Step    231: train                   accuracy |  0.92519647\n",
      "Step    231: train                       loss |  0.19421969\n",
      "Step    231: train         neg_log_perplexity |  0.19421968\n",
      "Step    231: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    231: eval                    accuracy |  0.92364776\n",
      "Step    231: eval                        loss |  0.19863413\n",
      "Step    231: eval          neg_log_perplexity |  0.19863410\n",
      "Step    231: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    231: Finished evaluation\n",
      "\n",
      "Step    241: Ran 10 train steps in 5.92 secs\n",
      "Step    241: Evaluation\n",
      "Step    241: train                   accuracy |  0.92920291\n",
      "Step    241: train                       loss |  0.18326724\n",
      "Step    241: train         neg_log_perplexity |  0.18326728\n",
      "Step    241: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    241: eval                    accuracy |  0.92875826\n",
      "Step    241: eval                        loss |  0.18556763\n",
      "Step    241: eval          neg_log_perplexity |  0.18556762\n",
      "Step    241: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    241: Finished evaluation\n",
      "\n",
      "Step    251: Ran 10 train steps in 5.92 secs\n",
      "Step    251: Evaluation\n",
      "Step    251: train                   accuracy |  0.93393004\n",
      "Step    251: train                       loss |  0.16747294\n",
      "Step    251: train         neg_log_perplexity |  0.16747296\n",
      "Step    251: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    251: eval                    accuracy |  0.93405044\n",
      "Step    251: eval                        loss |  0.16748261\n",
      "Step    251: eval          neg_log_perplexity |  0.16748263\n",
      "Step    251: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    251: Finished evaluation\n",
      "\n",
      "Step    261: Ran 10 train steps in 5.93 secs\n",
      "Step    261: Evaluation\n",
      "Step    261: train                   accuracy |  0.93854320\n",
      "Step    261: train                       loss |  0.15689532\n",
      "Step    261: train         neg_log_perplexity |  0.15689534\n",
      "Step    261: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    261: eval                    accuracy |  0.93806785\n",
      "Step    261: eval                        loss |  0.15750791\n",
      "Step    261: eval          neg_log_perplexity |  0.15750793\n",
      "Step    261: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    261: Finished evaluation\n",
      "\n",
      "Step    271: Ran 10 train steps in 5.95 secs\n",
      "Step    271: Evaluation\n",
      "Step    271: train                   accuracy |  0.94193625\n",
      "Step    271: train                       loss |  0.14457563\n",
      "Step    271: train         neg_log_perplexity |  0.14457563\n",
      "Step    271: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    271: eval                    accuracy |  0.94145215\n",
      "Step    271: eval                        loss |  0.14557867\n",
      "Step    271: eval          neg_log_perplexity |  0.14557865\n",
      "Step    271: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    271: Finished evaluation\n",
      "\n",
      "Step    281: Ran 10 train steps in 5.93 secs\n",
      "Step    281: Evaluation\n",
      "Step    281: train                   accuracy |  0.94495916\n",
      "Step    281: train                       loss |  0.13602555\n",
      "Step    281: train         neg_log_perplexity |  0.13602555\n",
      "Step    281: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    281: eval                    accuracy |  0.94507092\n",
      "Step    281: eval                        loss |  0.13578187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    281: eval          neg_log_perplexity |  0.13578185\n",
      "Step    281: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    281: Finished evaluation\n",
      "\n",
      "Step    291: Ran 10 train steps in 5.92 secs\n",
      "Step    291: Evaluation\n",
      "Step    291: train                   accuracy |  0.94935340\n",
      "Step    291: train                       loss |  0.12381007\n",
      "Step    291: train         neg_log_perplexity |  0.12381005\n",
      "Step    291: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    291: eval                    accuracy |  0.94912773\n",
      "Step    291: eval                        loss |  0.12447064\n",
      "Step    291: eval          neg_log_perplexity |  0.12447064\n",
      "Step    291: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    291: Finished evaluation\n",
      "\n",
      "Step    301: Ran 10 train steps in 5.92 secs\n",
      "Step    301: Evaluation\n",
      "Step    301: train                   accuracy |  0.95251644\n",
      "Step    301: train                       loss |  0.11484772\n",
      "Step    301: train         neg_log_perplexity |  0.11484773\n",
      "Step    301: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    301: eval                    accuracy |  0.95216811\n",
      "Step    301: eval                        loss |  0.11618605\n",
      "Step    301: eval          neg_log_perplexity |  0.11618604\n",
      "Step    301: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    301: Finished evaluation\n",
      "\n",
      "Step    311: Ran 10 train steps in 5.94 secs\n",
      "Step    311: Evaluation\n",
      "Step    311: train                   accuracy |  0.95467848\n",
      "Step    311: train                       loss |  0.10872594\n",
      "Step    311: train         neg_log_perplexity |  0.10872594\n",
      "Step    311: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    311: eval                    accuracy |  0.95428413\n",
      "Step    311: eval                        loss |  0.10976202\n",
      "Step    311: eval          neg_log_perplexity |  0.10976202\n",
      "Step    311: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    311: Finished evaluation\n",
      "\n",
      "Step    321: Ran 10 train steps in 5.94 secs\n",
      "Step    321: Evaluation\n",
      "Step    321: train                   accuracy |  0.95744503\n",
      "Step    321: train                       loss |  0.10201758\n",
      "Step    321: train         neg_log_perplexity |  0.10201757\n",
      "Step    321: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    321: eval                    accuracy |  0.95810878\n",
      "Step    321: eval                        loss |  0.10107217\n",
      "Step    321: eval          neg_log_perplexity |  0.10107215\n",
      "Step    321: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    321: Finished evaluation\n",
      "\n",
      "Step    331: Ran 10 train steps in 5.94 secs\n",
      "Step    331: Evaluation\n",
      "Step    331: train                   accuracy |  0.95931578\n",
      "Step    331: train                       loss |  0.09666274\n",
      "Step    331: train         neg_log_perplexity |  0.09666274\n",
      "Step    331: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    331: eval                    accuracy |  0.95932013\n",
      "Step    331: eval                        loss |  0.09661415\n",
      "Step    331: eval          neg_log_perplexity |  0.09661414\n",
      "Step    331: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    331: Finished evaluation\n",
      "\n",
      "Step    341: Ran 10 train steps in 5.94 secs\n",
      "Step    341: Evaluation\n",
      "Step    341: train                   accuracy |  0.96193993\n",
      "Step    341: train                       loss |  0.08980581\n",
      "Step    341: train         neg_log_perplexity |  0.08980581\n",
      "Step    341: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    341: eval                    accuracy |  0.96208894\n",
      "Step    341: eval                        loss |  0.08977766\n",
      "Step    341: eval          neg_log_perplexity |  0.08977766\n",
      "Step    341: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    341: Finished evaluation\n",
      "\n",
      "Step    351: Ran 10 train steps in 5.94 secs\n",
      "Step    351: Evaluation\n",
      "Step    351: train                   accuracy |  0.96339226\n",
      "Step    351: train                       loss |  0.08683258\n",
      "Step    351: train         neg_log_perplexity |  0.08683256\n",
      "Step    351: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    351: eval                    accuracy |  0.96294981\n",
      "Step    351: eval                        loss |  0.08842470\n",
      "Step    351: eval          neg_log_perplexity |  0.08842470\n",
      "Step    351: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    351: Finished evaluation\n",
      "\n",
      "Step    361: Ran 10 train steps in 5.93 secs\n",
      "Step    361: Evaluation\n",
      "Step    361: train                   accuracy |  0.96624434\n",
      "Step    361: train                       loss |  0.07948074\n",
      "Step    361: train         neg_log_perplexity |  0.07948073\n",
      "Step    361: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    361: eval                    accuracy |  0.96563536\n",
      "Step    361: eval                        loss |  0.08041948\n",
      "Step    361: eval          neg_log_perplexity |  0.08041948\n",
      "Step    361: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    361: Finished evaluation\n",
      "\n",
      "Step    371: Ran 10 train steps in 5.93 secs\n",
      "Step    371: Evaluation\n",
      "Step    371: train                   accuracy |  0.96838009\n",
      "Step    371: train                       loss |  0.07318555\n",
      "Step    371: train         neg_log_perplexity |  0.07318555\n",
      "Step    371: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    371: eval                    accuracy |  0.96827489\n",
      "Step    371: eval                        loss |  0.07424272\n",
      "Step    371: eval          neg_log_perplexity |  0.07424271\n",
      "Step    371: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    371: Finished evaluation\n",
      "\n",
      "Step    381: Ran 10 train steps in 5.94 secs\n",
      "Step    381: Evaluation\n",
      "Step    381: train                   accuracy |  0.96978414\n",
      "Step    381: train                       loss |  0.07046606\n",
      "Step    381: train         neg_log_perplexity |  0.07046606\n",
      "Step    381: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    381: eval                    accuracy |  0.96983016\n",
      "Step    381: eval                        loss |  0.07038535\n",
      "Step    381: eval          neg_log_perplexity |  0.07038537\n",
      "Step    381: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    381: Finished evaluation\n",
      "\n",
      "Step    391: Ran 10 train steps in 5.93 secs\n",
      "Step    391: Evaluation\n",
      "Step    391: train                   accuracy |  0.97059900\n",
      "Step    391: train                       loss |  0.06910590\n",
      "Step    391: train         neg_log_perplexity |  0.06910589\n",
      "Step    391: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    391: eval                    accuracy |  0.97090131\n",
      "Step    391: eval                        loss |  0.06799490\n",
      "Step    391: eval          neg_log_perplexity |  0.06799491\n",
      "Step    391: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    391: Finished evaluation\n",
      "\n",
      "Step    401: Ran 10 train steps in 5.94 secs\n",
      "Step    401: Evaluation\n",
      "Step    401: train                   accuracy |  0.97230107\n",
      "Step    401: train                       loss |  0.06470048\n",
      "Step    401: train         neg_log_perplexity |  0.06470048\n",
      "Step    401: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    401: eval                    accuracy |  0.97219586\n",
      "Step    401: eval                        loss |  0.06521214\n",
      "Step    401: eval          neg_log_perplexity |  0.06521215\n",
      "Step    401: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    401: Finished evaluation\n",
      "\n",
      "Step    411: Ran 10 train steps in 5.94 secs\n",
      "Step    411: Evaluation\n",
      "Step    411: train                   accuracy |  0.97363722\n",
      "Step    411: train                       loss |  0.06117208\n",
      "Step    411: train         neg_log_perplexity |  0.06117209\n",
      "Step    411: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    411: eval                    accuracy |  0.97393513\n",
      "Step    411: eval                        loss |  0.06001602\n",
      "Step    411: eval          neg_log_perplexity |  0.06001603\n",
      "Step    411: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    411: Finished evaluation\n",
      "\n",
      "Step    421: Ran 10 train steps in 5.95 secs\n",
      "Step    421: Evaluation\n",
      "Step    421: train                   accuracy |  0.97501069\n",
      "Step    421: train                       loss |  0.05798705\n",
      "Step    421: train         neg_log_perplexity |  0.05798705\n",
      "Step    421: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    421: eval                    accuracy |  0.97478074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    421: eval                        loss |  0.05848213\n",
      "Step    421: eval          neg_log_perplexity |  0.05848213\n",
      "Step    421: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    421: Finished evaluation\n",
      "\n",
      "Step    431: Ran 10 train steps in 5.94 secs\n",
      "Step    431: Evaluation\n",
      "Step    431: train                   accuracy |  0.97627461\n",
      "Step    431: train                       loss |  0.05556519\n",
      "Step    431: train         neg_log_perplexity |  0.05556519\n",
      "Step    431: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    431: eval                    accuracy |  0.97620451\n",
      "Step    431: eval                        loss |  0.05535147\n",
      "Step    431: eval          neg_log_perplexity |  0.05535147\n",
      "Step    431: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    431: Finished evaluation\n",
      "\n",
      "Step    441: Ran 10 train steps in 5.96 secs\n",
      "Step    441: Evaluation\n",
      "Step    441: train                   accuracy |  0.97735238\n",
      "Step    441: train                       loss |  0.05214088\n",
      "Step    441: train         neg_log_perplexity |  0.05214089\n",
      "Step    441: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    441: eval                    accuracy |  0.97760427\n",
      "Step    441: eval                        loss |  0.05137523\n",
      "Step    441: eval          neg_log_perplexity |  0.05137523\n",
      "Step    441: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    441: Finished evaluation\n",
      "\n",
      "Step    451: Ran 10 train steps in 5.94 secs\n",
      "Step    451: Evaluation\n",
      "Step    451: train                   accuracy |  0.97760648\n",
      "Step    451: train                       loss |  0.05249086\n",
      "Step    451: train         neg_log_perplexity |  0.05249086\n",
      "Step    451: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    451: eval                    accuracy |  0.97748816\n",
      "Step    451: eval                        loss |  0.05285563\n",
      "Step    451: eval          neg_log_perplexity |  0.05285563\n",
      "Step    451: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    451: Finished evaluation\n",
      "\n",
      "Step    461: Ran 10 train steps in 5.93 secs\n",
      "Step    461: Evaluation\n",
      "Step    461: train                   accuracy |  0.97867322\n",
      "Step    461: train                       loss |  0.04933032\n",
      "Step    461: train         neg_log_perplexity |  0.04933032\n",
      "Step    461: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    461: eval                    accuracy |  0.97871047\n",
      "Step    461: eval                        loss |  0.04938859\n",
      "Step    461: eval          neg_log_perplexity |  0.04938859\n",
      "Step    461: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    461: Finished evaluation\n",
      "\n",
      "Step    471: Ran 10 train steps in 5.94 secs\n",
      "Step    471: Evaluation\n",
      "Step    471: train                   accuracy |  0.98043227\n",
      "Step    471: train                       loss |  0.04497821\n",
      "Step    471: train         neg_log_perplexity |  0.04497821\n",
      "Step    471: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    471: eval                    accuracy |  0.98013431\n",
      "Step    471: eval                        loss |  0.04613326\n",
      "Step    471: eval          neg_log_perplexity |  0.04613326\n",
      "Step    471: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    471: Finished evaluation\n",
      "\n",
      "Step    481: Ran 10 train steps in 5.98 secs\n",
      "Step    481: Evaluation\n",
      "Step    481: train                   accuracy |  0.98122954\n",
      "Step    481: train                       loss |  0.04318549\n",
      "Step    481: train         neg_log_perplexity |  0.04318549\n",
      "Step    481: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    481: eval                    accuracy |  0.98139822\n",
      "Step    481: eval                        loss |  0.04351223\n",
      "Step    481: eval          neg_log_perplexity |  0.04351223\n",
      "Step    481: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    481: Finished evaluation\n",
      "\n",
      "Step    491: Ran 10 train steps in 5.95 secs\n",
      "Step    491: Evaluation\n",
      "Step    491: train                   accuracy |  0.98175967\n",
      "Step    491: train                       loss |  0.04241743\n",
      "Step    491: train         neg_log_perplexity |  0.04241743\n",
      "Step    491: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    491: eval                    accuracy |  0.98171586\n",
      "Step    491: eval                        loss |  0.04253148\n",
      "Step    491: eval          neg_log_perplexity |  0.04253148\n",
      "Step    491: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    491: Finished evaluation\n",
      "\n",
      "Step    501: Ran 10 train steps in 5.94 secs\n",
      "Step    501: Evaluation\n",
      "Step    501: train                   accuracy |  0.98238397\n",
      "Step    501: train                       loss |  0.04084034\n",
      "Step    501: train         neg_log_perplexity |  0.04084034\n",
      "Step    501: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    501: eval                    accuracy |  0.98293376\n",
      "Step    501: eval                        loss |  0.03971236\n",
      "Step    501: eval          neg_log_perplexity |  0.03971237\n",
      "Step    501: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    501: Finished evaluation\n",
      "\n",
      "Step    511: Ran 10 train steps in 5.94 secs\n",
      "Step    511: Evaluation\n",
      "Step    511: train                   accuracy |  0.98377705\n",
      "Step    511: train                       loss |  0.03738253\n",
      "Step    511: train         neg_log_perplexity |  0.03738253\n",
      "Step    511: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    511: eval                    accuracy |  0.98388439\n",
      "Step    511: eval                        loss |  0.03745554\n",
      "Step    511: eval          neg_log_perplexity |  0.03745554\n",
      "Step    511: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    511: Finished evaluation\n",
      "\n",
      "Step    521: Ran 10 train steps in 5.95 secs\n",
      "Step    521: Evaluation\n",
      "Step    521: train                   accuracy |  0.98452848\n",
      "Step    521: train                       loss |  0.03555402\n",
      "Step    521: train         neg_log_perplexity |  0.03555402\n",
      "Step    521: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    521: eval                    accuracy |  0.98460293\n",
      "Step    521: eval                        loss |  0.03575912\n",
      "Step    521: eval          neg_log_perplexity |  0.03575913\n",
      "Step    521: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    521: Finished evaluation\n",
      "\n",
      "Step    531: Ran 10 train steps in 5.94 secs\n",
      "Step    531: Evaluation\n",
      "Step    531: train                   accuracy |  0.98543972\n",
      "Step    531: train                       loss |  0.03409579\n",
      "Step    531: train         neg_log_perplexity |  0.03409580\n",
      "Step    531: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    531: eval                    accuracy |  0.98521852\n",
      "Step    531: eval                        loss |  0.03450247\n",
      "Step    531: eval          neg_log_perplexity |  0.03450247\n",
      "Step    531: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    531: Finished evaluation\n",
      "\n",
      "Step    541: Ran 10 train steps in 5.95 secs\n",
      "Step    541: Evaluation\n",
      "Step    541: train                   accuracy |  0.98540902\n",
      "Step    541: train                       loss |  0.03393660\n",
      "Step    541: train         neg_log_perplexity |  0.03393660\n",
      "Step    541: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    541: eval                    accuracy |  0.98527545\n",
      "Step    541: eval                        loss |  0.03396133\n",
      "Step    541: eval          neg_log_perplexity |  0.03396133\n",
      "Step    541: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    541: Finished evaluation\n",
      "\n",
      "Step    551: Ran 10 train steps in 5.94 secs\n",
      "Step    551: Evaluation\n",
      "Step    551: train                   accuracy |  0.98613846\n",
      "Step    551: train                       loss |  0.03235738\n",
      "Step    551: train         neg_log_perplexity |  0.03235738\n",
      "Step    551: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    551: eval                    accuracy |  0.98638815\n",
      "Step    551: eval                        loss |  0.03195552\n",
      "Step    551: eval          neg_log_perplexity |  0.03195552\n",
      "Step    551: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    551: Finished evaluation\n",
      "\n",
      "Step    561: Ran 10 train steps in 5.97 secs\n",
      "Step    561: Evaluation\n",
      "Step    561: train                   accuracy |  0.98711765\n",
      "Step    561: train                       loss |  0.02989600\n",
      "Step    561: train         neg_log_perplexity |  0.02989600\n",
      "Step    561: train weights_per_batch_per_core |  114129.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    561: eval                    accuracy |  0.98749000\n",
      "Step    561: eval                        loss |  0.02909270\n",
      "Step    561: eval          neg_log_perplexity |  0.02909270\n",
      "Step    561: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    561: Finished evaluation\n",
      "\n",
      "Step    571: Ran 10 train steps in 5.94 secs\n",
      "Step    571: Evaluation\n",
      "Step    571: train                   accuracy |  0.98787773\n",
      "Step    571: train                       loss |  0.02842221\n",
      "Step    571: train         neg_log_perplexity |  0.02842220\n",
      "Step    571: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    571: eval                    accuracy |  0.98789525\n",
      "Step    571: eval                        loss |  0.02837910\n",
      "Step    571: eval          neg_log_perplexity |  0.02837910\n",
      "Step    571: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    571: Finished evaluation\n",
      "\n",
      "Step    581: Ran 10 train steps in 5.96 secs\n",
      "Step    581: Evaluation\n",
      "Step    581: train                   accuracy |  0.98845601\n",
      "Step    581: train                       loss |  0.02670169\n",
      "Step    581: train         neg_log_perplexity |  0.02670169\n",
      "Step    581: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    581: eval                    accuracy |  0.98877370\n",
      "Step    581: eval                        loss |  0.02642236\n",
      "Step    581: eval          neg_log_perplexity |  0.02642236\n",
      "Step    581: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    581: Finished evaluation\n",
      "\n",
      "Step    591: Ran 10 train steps in 5.95 secs\n",
      "Step    591: Evaluation\n",
      "Step    591: train                   accuracy |  0.98887879\n",
      "Step    591: train                       loss |  0.02611302\n",
      "Step    591: train         neg_log_perplexity |  0.02611302\n",
      "Step    591: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    591: eval                    accuracy |  0.98903215\n",
      "Step    591: eval                        loss |  0.02586820\n",
      "Step    591: eval          neg_log_perplexity |  0.02586820\n",
      "Step    591: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    591: Finished evaluation\n",
      "\n",
      "Step    601: Ran 10 train steps in 5.95 secs\n",
      "Step    601: Evaluation\n",
      "Step    601: train                   accuracy |  0.99007046\n",
      "Step    601: train                       loss |  0.02365463\n",
      "Step    601: train         neg_log_perplexity |  0.02365464\n",
      "Step    601: train weights_per_batch_per_core |  114129.00000000\n",
      "Step    601: eval                    accuracy |  0.99005288\n",
      "Step    601: eval                        loss |  0.02366641\n",
      "Step    601: eval          neg_log_perplexity |  0.02366641\n",
      "Step    601: eval  weights_per_batch_per_core |  114129.00000000\n",
      "Step    601: Finished evaluation\n"
     ]
    }
   ],
   "source": [
    "# Train for 600 steps total\n",
    "# The first ~20 steps are slow to run, but after that it reaches steady-state\n",
    "# speed. This will take at least 30 minutes to run to completion, but can safely\n",
    "# be interrupted by selecting \"Runtime > Interrupt Execution\" from the menu.\n",
    "# The language model won't be exceptionally good when trained for just a few\n",
    "# steps and with minimal regularization. However, we can still sample from it to\n",
    "# see what it learns.\n",
    "trainer.train_epoch(n_steps=9, n_eval_steps=1)\n",
    "for _ in range(59):\n",
    "  trainer.train_epoch(n_steps=10, n_eval_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we report in the Reformer paper, increasing the number of hashing rounds\n",
    "# helps with quality. We can even increase the number of hashing rounds at\n",
    "# evaluation time only.\n",
    "gin.parse_config(\"\"\"LSHCausalAttention.n_hashes = 4\"\"\")\n",
    "model_infer = trax.models.ReformerLM(mode='predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a jitted copy of the model.\n",
    "jit_model_infer = trax.layers.base._accelerate(\n",
    "    model_infer._forward_internal, trax.math.device_count())\n",
    "# Set up the initial state for sampling.\n",
    "infer_state = model_infer.new_weights_and_state(\n",
    "    trax.supervised.trainer_lib.ShapeDtype((1,1), dtype=np.int32))[1]\n",
    "infer_state = trainer._for_n_devices(infer_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(length=2048, prompt=None):\n",
    "  \"\"\"Sample from the ReformerLM model\"\"\"\n",
    "  model_weights = trainer._opt_state[0][0]\n",
    "\n",
    "  # Token id 0 is the equivalent of a \"start\" token\n",
    "  cur_inputs = np.zeros((trax.math.device_count(), 1, 1), dtype=np.int32)\n",
    "\n",
    "  cur_state = infer_state\n",
    "  rngs = trax.math.random.split(trax.math.random.get_prng(0), trax.math.device_count())\n",
    "  all_samples = []\n",
    "\n",
    "  if prompt is not None:\n",
    "    prompt = np.asarray(\n",
    "        [TOKENIZER.EncodeAsIds(prompt)] * trax.math.device_count())\n",
    "\n",
    "  for iteration in range(length):\n",
    "    logits, cur_state = jit_model_infer(\n",
    "        cur_inputs,\n",
    "        model_weights,\n",
    "        cur_state,\n",
    "        rngs)\n",
    "    \n",
    "    if prompt is not None and iteration < prompt.shape[1]:\n",
    "      cur_samples = onp.array(prompt[:, iteration], dtype=int)\n",
    "    else:\n",
    "      logits = onp.array(logits)[:,0,0,:]\n",
    "      probs = onp.exp(logits)\n",
    "      cur_samples = [onp.random.choice(probs.shape[-1], p=probs[i,:])\n",
    "                     for i in range(probs.shape[0])]\n",
    "      cur_samples = onp.array(cur_samples, dtype=int)\n",
    "    all_samples.append(cur_samples)\n",
    "\n",
    "    cur_inputs = np.array(cur_samples[:,None,None])\n",
    "  all_samples = onp.stack(all_samples, -1)\n",
    "  \n",
    "  return all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "मिथिंगाखौ बे, जागासिनो दं।गोथैसालियाव थांनानै दं। गलिया। गभनि सलʼनि सलʼनि खन्थाइनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनि सलʼनो बेसेबा गोबाव खालामफायो। आरʼनि बिलाइयाव मोन्नायखाय कल्पʼनि बादʼवाव आगान होलाङो सुजुग्राया- \"आ हामखांब्लानो सिनायनो हायो। नों बबेवेटारवा, मुथाङै- \"आ हांमाʼ गोर्लैबादि आं खामानिखौ गारनो हाया। सुथि मोनखाङो। नाथाय बेखौ सलʼ लेवा गावनि मासियाव जिरायनानै सानो। सुथि मोनाबा बे बरʼवा गावनि मासियाव जिरायनानै सानो। अख्राङाव दोनफिनगासिनो दं।\" \"नोंथाङा बायहेरायाव ओंखार\n",
      "\n",
      "\n",
      "मिथिंगाखौ बे समाव अजितनो खिन्थाहैनाय खौरांखौ खिन्थानो मोननाय मुस्रीमा ? शान्तिया। गुमुर गोयै बिफाखौ लानानै इम्पʼनि उनावनोार्सखौ खेवनायनि सिमां ...नों नʼदों। बिफानि आथिङाव गसंना सारजों मोजाङै नोजोर होखानाय मैहुरारी नङाब्ला जाथाया बरʼवारीया सासे नार्सिंब्लाबो गोरोन्थिफोर थांलाय फैलाय खामानि मावग्रा आइजोआ बोहैथिखौ नुखायो नामा ?\" \"औ।\" बुंना हां बोयो। \"सारबोदों आं मोजाङै नोजोर होगासिनो दंमोन।\" बुंना बोहैथिया। सासे देहा बै ?\" \"मा मोनखो बेखौ मोनथिया। \"सोरनि मुलिखौ नायनानै बुङो। \"औ।\" बुंना बोहैथिया। \"एमदबनानै। सा गाज्रि मोनो डा. एस. ब्रह्मनि ?\" \"केमिष्टेबलʼ।\" \"जों थाखो लोगोनि एसेबां अनागारि ? समनि आंखालाव। \"मानो ? समनि आंखालाव बि. ब्रह्मवा। नाथाय गयथिया ?\" \"बे मा।\" \"बेयो। नाथाय गयथिया जेबो।\" हांखुर खालामबाय।\" \"रखाय मास्क होयोमोन रुमाव फैनानै बुंनायाव सुजुग्राया बोहैथिनि सिगाङाव सुजुग्राया। नाथाय मानो ?\" \"राव\n",
      "\n",
      "\n",
      "मिथिंगाखौ बे मुलुगाव अजितनो मुलियाव ?\" \"आं सानो।\" \"आं ? दाखालि गयथिखौ बुथारनायनि जाथाइखौ आं बिथां डनिफ्राय सिखारनानैसुजुग्राया बोहैथिनाजानायनो आं सानसेखालि अरखिया खुनुरोखोमै हाबफै।\" \"खननि फन खालामथʼनि थारखौ गयथिनि रुमनिफ्राय। नङाब्ला सानिफ्राय गयथिया सानसेखालि अरखिया खुनुरोखोमै बुंफिनदोंमोन। नङाब्ला फानहरनो सरकार रुम फारसे लाबोयो। नङाब्ला नोंनि गयथिखौ सुथारनाय समाव गाबख्रावजानो गिनानै खुगाखौ गयथिनि उनाव गिहांदोंमोन।\" \"जागोन।\" \"अहʼनाया बे बुब्लियावनो खनसायनि उनावनो अरखिया खुनुरोखोमै बुंनानै - सुजुग्राखौ लेंहरनानै रुम फारसे नायहरबाय थायो। रुम फारसे नायहरबाय थायो। रुम फारसे नायहरबाय थायो खनसायखौ खनसायखौ गानहोयो साथाम मोनलाखनो हाया। आं। नङाब्ला नोंनि रुमालजों बोहैथि मोननि नʼवाव फैनानै सिनायनाय जाबाय। खोनासंरिखौ जाथाइ जानाय सुबुं। नङाब्ला नोंनि रुमालजों माब्लाबाबो जोंनि आखायाव लानानै रुमाव हाबलाङो। नङाब्ला जाथाया जाब्ला लोगो लोगो अरखि ब्रह्मनि रुमाव उन्दुबाय थानाय एमाव फुथुनानै होयो बिमानोथोसै। बेनिखायनो \"बेनि अनगायैबो गय\n",
      "\n",
      "\n",
      "मिथिंगाखौ बे मुलुगाव अजितजों डाःखौ बेसुखौ बेसुखौ मोनैनिख्रावनायखौ। ? नुमारनो हागौ।जाहार बिथां डेननि मेगननिफ्राय मोदै गग्रायाव गसंथʼनि मेगननिफ्राय मोदै गग्रानि बाटामाव हमो।स्राथिं नायहरबाय थायो। खेबसे नायलायनायखौ बारा गोजान। बेनिखायनो बौदायानो जाबाय। नाथाय दरखौ नायगिरनानै मोजाङै नोजोर होबाय।जाहोना अजितखौ बुङो- \"नोंथांनि उनावनो सासे आइजोआ फैनानै बुङो- \"नोंथांनि मुलिखौ साजा माबा सानहाबबाय थादोंमोन।दा -हेफथि बरʼनि मुलिखौ बथल लान्दो नायगिरदों। हरफिनगासिनो दं।\" \"आइ एम छरि। जों नाजागोन \"ह ! बुङो- \"नोंथांनि मुङा ?\" \"बेनि रोखा।\" \"नोंथांनि मुलिखौ बुंफिनो- \"नोंथांनि मुलिखौ नायनानै। \"न गोनां जायोब्ला नोंथाङा अरखि ब्रह्मवाबो गसंना लाय रुम फारसे। \"नालासेनो खोन्था होनानै बुंफिनो- \"हाजʼवारी आरो।\" \"नोंथांनि मुङा ?\" \"बदि बुंनानै बिजिरसालिनि खोनासंग्राफोरखौ खानो हासिगौ।\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample from the Reformer language model, given a prefix.\n",
    "samples = sample(length=200, prompt=\"मिथिंगा\")\n",
    "for ids in samples:\n",
    "  print(TOKENIZER.DecodeIds(ids.tolist()))\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
